<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><script type="text/javascript" src="https://file.bkxsj.com/skin/book/js/sk.js"></script><meta name="robots" content="index,follow"><title>并行计算导论[PDF|Epub|txt|kindle电子书版本网盘下载]-灵感之桥</title><meta name="Keywords" content="并行计算导论"/><meta name="description" content="并行计算导论pdf下载文件大小为33MB,PDF页数为657页"/><meta http-equiv="X-UA-Compatible" content="IE=9; IE=8; IE=7; IE=EDGE;chrome=1"><link type="image/x-icon" rel="shortcut icon" href="https://www.shukui.net/skin/book/images/favicon.ico"><link type="text/css" rel="stylesheet" href="https://www.shukui.net/skin/book/css/style.css"><style>#main .d-main {margin-left: 0;width: 620px;}.down-btn {animation: myShake 2.5s linear .15s infinite}@keyframes myShake {0%, 66% {transform: translateZ(0)}67%, 73.6%, 83.6%, 93.6%, to {animation-timing-function: cubic-bezier(.215, .61, .355, 1);transform: translateZ(0)}80.3%, 81.4% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -4px, 0)}90.3% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -2px, 0)}97% {transform: translate3d(0, -.5px, 0)}}.copylink-btn {margin-right: 20px;}.copymd5-btn {margin-bottom: 25px;margin-left: 10px;}</style></head><body><div id="header"><div class="inner"><div class="logo"><a href="/"><img width="103" height="25" alt="灵感之桥"src="https://www.shukui.net/skin/book/images/logo.png"></a></div><div class="search"><form action="/so/search.php" target="_blank"><input type="text" autocomplete="off" id="bdcsMain" name="q" placeholder="书名 / 作者 / 出版社 / ISBN"class="inp-txt"><select class="inp-select" id="datasource" onchange="selectDatasource(this)"><option value="so">主库</option><option value="s">从库</option></select><input type="submit" value="搜索" class="inp-btn"></form></div></div></div><div id="main"><div class="d-main"><div class="tit"><h3>图书介绍</h3></div><h1 class="book-name">并行计算导论PDF|Epub|txt|kindle电子书版本网盘下载</h1><div class="d-info"><div class="b-thumb"><img src="https://www.shukui.net/cover/55/32381976.jpg" alt="并行计算导论"></div><div class="b-info"><ul><li>（美）格兰马（Grama，A.）著 著</li><li>出版社： 北京：机械工业出版社</li><li>ISBN：7111125126</li><li>出版时间：2003</li><li>标注页数：636页</li><li>文件大小：33MB</li><li>文件页数：657页</li><li>主题词：并行算法－高等学校－教材－英文</li></ul></div></div><div class="tit"><h3>PDF下载</h3></div><div></br><a style="color:red;" rel="external nofollow" href="https://www.kjlm.net/ebook/1934809.html"target="_blank"><b>点此进入-本书在线PDF格式电子书下载【推荐-云解压-方便快捷】直接下载PDF格式图书。移动端-PC端通用</a></b></br><a class="down-btn" rel="external nofollow" href="https://down.trackerbk.com/bt/06/32381976.torrent"target="_blank">种子下载</a>[BT下载速度快]温馨提示：（请使用BT下载软件FDM进行下载）<a rel="nofollow" href="https://www.freedownloadmanager.org/zh/" target="_blank">软件下载地址页</a><a class="down-btn" rel="external nofollow" href="https://down.p2spdb.com/06/32381976.rar" target="_blank">直链下载</a>[便捷但速度慢]&nbsp;&nbsp;<a style="color:red;" rel="external nofollow" href="https://pdfyl.ertongbook.com/44/32381976.pdf" target="_blank"><b>[在线试读本书]</b></a>&nbsp;&nbsp;<b> <a style="color:red;" rel="external nofollow" href="https://web.jyjl.org/index/recovery.html" target="_blank">[在线获取解压码]</a></b><div class="copymd5-btn"><a href="javascript:copyToClip('73f615df1220a952eabed40d5c1db4ac')">点击复制MD5值：73f615df1220a952eabed40d5c1db4ac</a></div></div><div class="tit"><h3>下载说明</h3></div><div style="margin:20px 10px"><h2>并行计算导论PDF格式电子书版下载</h2>下载的文件为RAR压缩包。需要使用解压软件进行解压得到PDF格式图书。<br><br><div class="copymd5-btn"><a href="javascript:copyToClip('magnet:?xt=urn:btih:RX6G6JQ2LJW7PBBEDNUSCQM7ICDMHIWK')">点击复制85GB完整离线版磁力链接到迅雷FDM等BT下载工具进行下载</a>&nbsp;&nbsp;<a rel="nofollow" target="_blank">详情点击-查看共享计划</a></div>建议使用BT下载工具Free Download Manager进行下载,简称FDM(免费,没有广告,支持多平台）。本站资源全部打包为BT种子。所以需要使用专业的BT下载软件进行下载。如BitComet qBittorrent uTorrent等BT下载工具。迅雷目前由于本站不是热门资源。不推荐使用！后期资源热门了。安装了迅雷也可以迅雷进行下载！<br><br><b>（文件页数 要大于 标注页数，上中下等多册电子书除外）</b><br><br><p style="color:red;"> <b>注意：本站所有压缩包均有解压码：</b> <a rel="nofollow" target="_blank"><b>点击下载压缩包解压工具</b></a></p></div><div class="tit"><h3>图书目录</h3></div><div id="book-contents"><p>CHAPTER 1 Introduction to Parallel Computing1</p><p>1.1 Motivating Parallelism2</p><p>1.1.1 The Computational Power Argument-from Transistors to FLOPS2</p><p>1.1.2 The Memory/Disk Speed Argument3</p><p>1.1.3 The Data Communication Argument4</p><p>1.2 Scope of Parallel Computing4</p><p>1.2.1 Applications in Engineering and Design4</p><p>1.2.2 Scientific Applications5</p><p>1.2.3 Commercial Applications5</p><p>1.2.4 Applications in Computer Systems6</p><p>1.3 Organization and Contents of the Text6</p><p>1.4 Bibliographic Remarks8</p><p>Problems9</p><p>CHAPTER 2 Parallel Progrmming Platforms11</p><p>2.1.1 Pipelining and Superscalar Execution12</p><p>2.1 Implicit parallelism:Trends in Microprocessor Architectures12</p><p>2.1.2 Very Long Instruction Word Processors15</p><p>2.2 Limitations of Memory System Performance16</p><p>2.2.1 Improving Effective Memory Latency Using Caches17</p><p>2.2.2 Impact of Memory Bandwidth18</p><p>2.2.3 Alternate Approaches for Hiding Memory Latency21</p><p>2.2.4 Tradeoffs of Multithreading and Prefetching23</p><p>2.3 Dichotomy of Parallel Computing Platforms24</p><p>2.3.1 Control Structure of parallel Platforms25</p><p>2.3.2 Communication Model of Parallel Platforms27</p><p>2.4 Physical Organization of Parallel Platforms31</p><p>2.4.1 Architecture of an Ideal Parallel Computer31</p><p>2.4.2 Interconnection Networks for Parallel Computers32</p><p>2.4.3 Network Topologies33</p><p>2.4.4 Evaluating Static Interconnection Networks43</p><p>2.4.5 Evaluating Dynamic Interconnection Networks44</p><p>2.4.6 Cache Coherence in Multiprocessor Systems45</p><p>2.5.1 Message Passing Costs in Parallel Computers53</p><p>2.5 Communication Costs in Parallel Machines53</p><p>2.5.2 Communication Costs in Shared-Address-Space Machines61</p><p>2.6 Routing Mechanisms for Interconnection Networks63</p><p>2.7 Impact of Process-Processor Mapping and Mapping Techniques65</p><p>2.7.1 Mapping Techniques for Graphs66</p><p>2.7.2 Cost-Performance Tradeoffs73</p><p>2.8 Bibliographic Remarks74</p><p>Problems76</p><p>CHAPTER 3 Principles of Parallel Algorithm Design85</p><p>3.1 Preliminaries86</p><p>3.1.1 Decomposition,Tasks,and Dependency Graphs86</p><p>3.1.2 Granularity,Concurrency,and Task-Interaction89</p><p>3.1.3 Processes and Mapping93</p><p>3.1.4 Processes versus Processors94</p><p>3.2 Decomposition Techniques95</p><p>3.2.1 Recursive Decomposition95</p><p>3.2.2 Data Decomposition97</p><p>3.2.3 Exploratory Decomposition105</p><p>3.2.4 Speculative Decomposition107</p><p>3.2.5 Hybrid Decompositions109</p><p>3.3 Characteristics of Tasks and Interactions110</p><p>3.3.1 Characteristics of Tasks110</p><p>3.3.2 Characteristics of Inter-Task Interactions112</p><p>3.4 Mapping Techniques for Load Balancing115</p><p>3.4.1 Schemes for Static Mapping117</p><p>3.4.2 Schemes for Dynamic Mapping130</p><p>3.5.1 Maximizing Data Locality132</p><p>3.5 Methods for Containing Interaction Overheads132</p><p>3.5.2 Minimizing Contention and Hot Spots134</p><p>3.5.3 Overlapping Computations with Interactions135</p><p>3.5.4 Replicating Data or Computations136</p><p>3.5.5 Using Optimized Collective Interaction Operations137</p><p>3.5.6 Overlapping Interactions with Other Interactions138</p><p>3.6 Parallel Algorithm Models139</p><p>3.6.1 The Data-Parallel Model139</p><p>3.6.3 Tht Work Pool Model140</p><p>3.6.2 The Task Graph Model140</p><p>3.6.4 The Master-Slave Model141</p><p>3.6.5 The Pipeline or Producer-Consumer Model141</p><p>3.6.6 Hybrid Models142</p><p>3.7 Bibliographic Remarks142</p><p>Problems143</p><p>CHAPTER 4 Basic Communication Operations147</p><p>4.1 One-to-All Broadcast and All-to-One Reduction149</p><p>4.1.1 Ring or Linear Array149</p><p>4.1.2 Mesh152</p><p>4.1.3 Hypercube153</p><p>4.1.4 Balanced Binary Tree153</p><p>4.1.5 Detailed Algorithms154</p><p>4.1.6 Cost Analysis156</p><p>4.2 All-to-All Broadcast and Reduction157</p><p>4.2.1 Linear Array and Ring158</p><p>4.2.2 Mesh160</p><p>4.2.3 Hypercube161</p><p>4.2.4 Cost Analysis164</p><p>4.3 All-Reduce and Prefix-Sum Operations166</p><p>4.4 Scatter and Gather167</p><p>4.5 All-to-All Personalized Communication170</p><p>4.5.1 Ring173</p><p>4.5.2 Mesh174</p><p>4.5.3 Hypercube175</p><p>4.6.1 Mesh179</p><p>4.6 Circular Shift179</p><p>4.6.2 Hypercube181</p><p>4.7 Improving the Speed of Some Communication Operations184</p><p>4.7.1 Splitting and Routing Messages in Parts184</p><p>4.7.2 All-Port Communication186</p><p>4.8 Summary187</p><p>4.9 Bibliographic Remarks188</p><p>Problems190</p><p>5.1 Sources of Overhead in Parallel Programs195</p><p>CHAPTER 5 Analytical Modeling of Parallel Programs195</p><p>5.2 Performance Metrics for Parallel Systems197</p><p>5.2.1 Execution Time197</p><p>5.2.2 Total Parallel Overhead197</p><p>5.2.3 Speedup198</p><p>5.2.4 Efficiency202</p><p>5.2.5 Cost203</p><p>5.3 The Effect of Granularity on Performance205</p><p>5.4 Scalability of Parallel Systems208</p><p>5.4.1 Scaling Characteristics of Parallel Programs209</p><p>5.4.2 The Isoefficiency Metric of Scalability212</p><p>5.4.3 Cost-Optimality and the Isoefficiency Function217</p><p>5.4.4 A Lower Bound on the Isoefficiency Function217</p><p>5.4.5 The Degree of Concurrency and the Isoefficiency Function218</p><p>5.5 Minimum Execution Time and Minimum Cost-Optimal Execution Time218</p><p>5.6 Asymptotic Analysis of Parallel Programs221</p><p>5.7 Other Scalability Metrics222</p><p>5.8 Bibliographic Remarks226</p><p>Problems228</p><p>CHAPTER 6 Programming Using the Message-Passing Paradigm233</p><p>6.1 Principles of Message-Passing Programming233</p><p>6.2 The Building Blocks:Send and Receive Operations235</p><p>6.2.1 Blocking Message Passing Operations236</p><p>6.2.2 Non-Blocking Message Passing Operations239</p><p>6.3 MPI:the Message Passing Interface240</p><p>6.3.2 Communicators242</p><p>6.3.1 Starting and Terminating the MPI Library242</p><p>6.2.3 Getting Information243</p><p>6.3.4 Sending and Receiving Messages244</p><p>6.3.5 Example:Odd-Even Sort248</p><p>6.4 Topologies and Embedding250</p><p>6.4.1 Creating and Using Cartesian Topologies251</p><p>6.4.2 Example:Cannon s Matrix-Matrix Multiplication253</p><p>6.5 Overlapping Communication with Computation255</p><p>6.5.1 Non-Blocking Communication Operations255</p><p>6.6.2 Broadcast260</p><p>6.6.1 Barrier260</p><p>6.6 Collective Communication and Computation Operations260</p><p>6.6.3 Reduction261</p><p>6.6.4 Prefix263</p><p>6.6.5 Gather263</p><p>6.6.6 Scatter264</p><p>6.6.7 All-to-All265</p><p>6.6.8 Example:One-Dimensional Matrix-Vector Multiplication266</p><p>6.6.9 Example:Single-Source Shortest-Path268</p><p>6.6.10 Example:Sample Sort270</p><p>6.7 Groups and Communicators272</p><p>6.7.1 Example:Two-Dimensional Matrix-Vector Multiplication274</p><p>6.8 Bibliographic Remarks276</p><p>Problems277</p><p>CHAPTER 7 Programming Shared Address Space Platforms279</p><p>7.1 Thread Basics280</p><p>7.2 Why Threads?281</p><p>7.4 Thread Basics:Creation and Termination282</p><p>7.3 The POSIX Thread API282</p><p>7.5 Synchronization Primitives in Pthreads287</p><p>7.5.1 Mutual Exclusion for Shared Variables287</p><p>7.5.2 Condition Variables for Synchronization294</p><p>7.6 Controlling Thread and Synchronization Attributes298</p><p>7.6.1 Attributes Objects for Threads299</p><p>7.6.2 Attributes Objects for Mutexes300</p><p>7.7 Thread Cancellation301</p><p>7.8.1 Read-Write Locks302</p><p>7.8 Composite Synchronization Constructs302</p><p>7.8.2 Barriers307</p><p>7.9 Tips for Designing Asynchronous Programs310</p><p>7.10 OpenMP:a Standard for Directive Based Parallel Programming311</p><p>7.10.1 The OpenMP Programming Model312</p><p>7.10.2 Specifying Concurrent Tasks in OpenMP315</p><p>7.10.3 Synchronization Constructs in OpenMP322</p><p>7.10.4 Data Handling in OpenMP327</p><p>7.10.5 OpenMP Library Functions328</p><p>7.10.6 Environment Variables in OpenMP330</p><p>7.10.7 Explicit Threads versus OpenMP Based Programming331</p><p>7.11 Bibliographic Remarks332</p><p>Problems332</p><p>CHAPTER 8 Dense Matrix Algorithms337</p><p>8.1 Matrix-Vector Multiplication337</p><p>8.1.1 Rowwise 1-D Partitioning338</p><p>8.1.2 2-D Partitioning341</p><p>8.2 Matrix-Matrix Multiplication345</p><p>8.2.1 A Simple Parallel Algorithm346</p><p>8.2.2 Cannon s Algorithm347</p><p>8.2.3 The DNS Algorithm349</p><p>8.3 Solving a System of Linear Equations352</p><p>8.3.1 A Simple Gaussian Elimination Algorithm353</p><p>8.3.2 Gaussian Elimination with Partial Pivoting366</p><p>8.3.3 Solving a Triangular System:Back-Substitution369</p><p>8.3.4 Numerical Considerations in Solving Systems of Linear Equations370</p><p>8.4 Bibliographic Remarks371</p><p>Problems372</p><p>CHAPTER 9 Sorting379</p><p>9.1 Issues in Sorting on Parallel Computers380</p><p>9.1.1 Where the Input and Output Sequences are Stored380</p><p>9.1.2 How Comparisons are Performed380</p><p>9.2 Sorting Networks382</p><p>9.2.1 Bitonic Sort384</p><p>9.2.2 Mapping Bitonic Sort to a Hypercube and a Mesh387</p><p>9.3 Bubble Sort and its Variants394</p><p>9.3.1 Odd-Even Transposition395</p><p>9.3.2 Shellsort398</p><p>9.4 Quicksort399</p><p>9.4.1 Parallelizing Quicksort401</p><p>9.4.2 Parallel Formulation for a CRCW PRAM402</p><p>9.4.3 Parallel Formulation for Practical Architectures404</p><p>9.4.4 Pivot Selection411</p><p>9.5 Bucket and Sample Sort412</p><p>9.6.1 Enumeration Sort414</p><p>9.6 Other Sorting Algorithms414</p><p>9.6.2 Radix Sort415</p><p>9.7 Bibliographic Remarks416</p><p>Problems419</p><p>CHAPTER 10 Graph Algorithms429</p><p>10.1 Definitions and Representation429</p><p>10.2 Minimum Spanning Tree:Prim s Algorithm432</p><p>10.3 Single-Source Shortest Paths:Dijkstra s Algorithm436</p><p>10.4 All-Pairs Shortest Paths437</p><p>10.4.1 Dijkstra s Algorithm438</p><p>10.4.2 Floyd s Algorithm440</p><p>10.4.3 Performance Comparisons445</p><p>10.5 Transitive Closure445</p><p>10.6 Connected Components446</p><p>10.6.1 A Depth-First Search Based Algorithm446</p><p>10.7 Algorithms for Sparse Graphs450</p><p>10.7.1 Finding a Maximal Independent Set451</p><p>10.7.2 Single-Source Shortest Paths455</p><p>10.8 Bibliographic Remarks462</p><p>Problems465</p><p>CHAPTER 11 Search Algorithms for Discrete Optimization Problems469</p><p>11.1 Definitions and Examples469</p><p>11.2 Sequential Search Algorithms474</p><p>11.2.1 Depth-First Search Algorithms474</p><p>11.2.2 Best-First Search Algorithms478</p><p>11.3 Search Overhead Factor478</p><p>11.4 Parallel Depth-First Search480</p><p>11.4.1 Important Parameters of Parallel DFS482</p><p>11.4.2 A General Framework for Analysis of Parallel DFS485</p><p>11.4.3 Analvsis of Load-Balancing Schemes488</p><p>11.4.4 Termination Detection490</p><p>11.4.5 Experimental Results492</p><p>11.4.6 Parallel Formulations of Depth-First Branch-and-Bound Search495</p><p>11.4.7 Parallel Formulations of IDA496</p><p>11.5 Parallel Best-First Search496</p><p>11.6 Speedup Anomalies in Parallel Search Algorithms501</p><p>11.6.1 Analysis of Average Speedup in Parallel DFS502</p><p>11.7 Bibliographic Remarks505</p><p>Problems510</p><p>CHAPTER 12 Dynamic Programming515</p><p>12.1 Overview of Dynamic Programming515</p><p>12.2 Serial Monadic DP Formulations518</p><p>12.2.1 The Shortest-Path Problem518</p><p>12.2.2 The O/I Knapsack Problem520</p><p>12.3 Nonserial Monadic DP Formulations523</p><p>12.3.1 The Longest-Common-Subsequence Problem523</p><p>12.4 Serial Polyadic DP Formulations526</p><p>12.4 1 Floyd s All-Pairs Shortest-Paths Algorithm526</p><p>12.5 Nonserial Polyadic DP Formulations527</p><p>12.5.1 The Optimal Matrix-Parenthesization Problem527</p><p>12.6 Summary and Discussion530</p><p>12.7 Bibliographic Remarks531</p><p>Problems532</p><p>CHAPTER 13 Fast Fourier Transform537</p><p>13.1 The Serial Algorithm538</p><p>13.2 The Binary-Exchange Algorithm541</p><p>13.2.1 A Full Bandwidth Network541</p><p>13.2.2 Limited Bandwidth Network548</p><p>13.2.3 Extra Computations in Parallel FFT551</p><p>13.3 The Transpose Algorithm553</p><p>13.3.1 Two-Dimensional Transpose Algorithm553</p><p>13.3.2 The Generalized Transpose Algorithm556</p><p>13.4 Bibliographic Remarks560</p><p>Problems562</p><p>APPENDIX A Complexity of Functions and Order Analytsis565</p><p>A.1 Complexity of Functions565</p><p>A.2 Order Analysis of Functions566</p><p>Bibliography569</p><p>Author Index611</p><p>Subject Index621</p><p></p></div></div><div class="d-rt"><h3>热门推荐</h3><ul><li><a href="/book/1582206.html">1582206.html</a></li><li><a href="/book/515084.html">515084.html</a></li><li><a href="/book/2358323.html">2358323.html</a></li><li><a href="/book/2471555.html">2471555.html</a></li><li><a href="/book/46641.html">46641.html</a></li><li><a href="/book/1507897.html">1507897.html</a></li><li><a href="/book/660082.html">660082.html</a></li><li><a href="/book/2871955.html">2871955.html</a></li><li><a href="/book/1002504.html">1002504.html</a></li><li><a href="/book/2240365.html">2240365.html</a></li></ul></div></div><div id="footer"><p>Copyright&nbsp;&copy;&nbsp;2025&nbsp;&nbsp;<a href="/list/">最新更新</a></p><p>请使用FDM BitComet qBittorrent uTorrent等BT下载工具，下载本站电子书资源！首推Free Download Manager下载软件。文件页数>标注页数[分册图书除外]</p></div></body></html>