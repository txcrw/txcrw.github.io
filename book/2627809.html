<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><script type="text/javascript" src="https://file.bkxsj.com/skin/book/js/sk.js"></script><meta name="robots" content="index,follow"><title>神经网络与机器学习 英文版[PDF|Epub|txt|kindle电子书版本网盘下载]-灵感之桥</title><meta name="Keywords" content="神经网络与机器学习 英文版"/><meta name="description" content="神经网络与机器学习 英文版pdf下载文件大小为85MB,PDF页数为939页"/><meta http-equiv="X-UA-Compatible" content="IE=9; IE=8; IE=7; IE=EDGE;chrome=1"><link type="image/x-icon" rel="shortcut icon" href="https://www.shukui.net/skin/book/images/favicon.ico"><link type="text/css" rel="stylesheet" href="https://www.shukui.net/skin/book/css/style.css"><style>#main .d-main {margin-left: 0;width: 620px;}.down-btn {animation: myShake 2.5s linear .15s infinite}@keyframes myShake {0%, 66% {transform: translateZ(0)}67%, 73.6%, 83.6%, 93.6%, to {animation-timing-function: cubic-bezier(.215, .61, .355, 1);transform: translateZ(0)}80.3%, 81.4% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -4px, 0)}90.3% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -2px, 0)}97% {transform: translate3d(0, -.5px, 0)}}.copylink-btn {margin-right: 20px;}.copymd5-btn {margin-bottom: 25px;margin-left: 10px;}</style></head><body><div id="header"><div class="inner"><div class="logo"><a href="/"><img width="103" height="25" alt="灵感之桥"src="https://www.shukui.net/skin/book/images/logo.png"></a></div><div class="search"><form action="/so/search.php" target="_blank"><input type="text" autocomplete="off" id="bdcsMain" name="q" placeholder="书名 / 作者 / 出版社 / ISBN"class="inp-txt"><select class="inp-select" id="datasource" onchange="selectDatasource(this)"><option value="so">主库</option><option value="s">从库</option></select><input type="submit" value="搜索" class="inp-btn"></form></div></div></div><div id="main"><div class="d-main"><div class="tit"><h3>图书介绍</h3></div><h1 class="book-name">神经网络与机器学习 英文版PDF|Epub|txt|kindle电子书版本网盘下载</h1><div class="d-info"><div class="b-thumb"><img src="https://www.shukui.net/cover/1/33412064.jpg" alt="神经网络与机器学习 英文版"></div><div class="b-info"><ul><li>(加)SimonHaykin著 著</li><li>出版社： 北京：机械工业出版社</li><li>ISBN：9787111265283</li><li>出版时间：2009</li><li>标注页数：906页</li><li>文件大小：85MB</li><li>文件页数：939页</li><li>主题词：人工神经－神经网络－英文；机器学习－英文</li></ul></div></div><div class="tit"><h3>PDF下载</h3></div><div></br><a style="color:red;" rel="external nofollow" href="https://www.kjlm.net/ebook/2627813.html"target="_blank"><b>点此进入-本书在线PDF格式电子书下载【推荐-云解压-方便快捷】直接下载PDF格式图书。移动端-PC端通用</a></b></br><a class="down-btn" rel="external nofollow" href="https://down.trackerbk.com/bt/07/33412064.torrent"target="_blank">种子下载</a>[BT下载速度快]温馨提示：（请使用BT下载软件FDM进行下载）<a rel="nofollow" href="https://www.freedownloadmanager.org/zh/" target="_blank">软件下载地址页</a><a class="down-btn" rel="external nofollow" href="https://down.p2spdb.com/07/33412064.rar" target="_blank">直链下载</a>[便捷但速度慢]&nbsp;&nbsp;<a style="color:red;" rel="external nofollow" href="https://pdfyl.ertongbook.com/61/33412064.pdf" target="_blank"><b>[在线试读本书]</b></a>&nbsp;&nbsp;<b> <a style="color:red;" rel="external nofollow" href="https://web.jyjl.org/index/recovery.html" target="_blank">[在线获取解压码]</a></b><div class="copymd5-btn"><a href="javascript:copyToClip('44dc958419d03fb41cb08361e1564a48')">点击复制MD5值：44dc958419d03fb41cb08361e1564a48</a></div></div><div class="tit"><h3>下载说明</h3></div><div style="margin:20px 10px"><h2>神经网络与机器学习 英文版PDF格式电子书版下载</h2>下载的文件为RAR压缩包。需要使用解压软件进行解压得到PDF格式图书。<br><br><div class="copymd5-btn"><a href="javascript:copyToClip('magnet:?xt=urn:btih:RX6G6JQ2LJW7PBBEDNUSCQM7ICDMHIWK')">点击复制85GB完整离线版磁力链接到迅雷FDM等BT下载工具进行下载</a>&nbsp;&nbsp;<a rel="nofollow" target="_blank">详情点击-查看共享计划</a></div>建议使用BT下载工具Free Download Manager进行下载,简称FDM(免费,没有广告,支持多平台）。本站资源全部打包为BT种子。所以需要使用专业的BT下载软件进行下载。如BitComet qBittorrent uTorrent等BT下载工具。迅雷目前由于本站不是热门资源。不推荐使用！后期资源热门了。安装了迅雷也可以迅雷进行下载！<br><br><b>（文件页数 要大于 标注页数，上中下等多册电子书除外）</b><br><br><p style="color:red;"> <b>注意：本站所有压缩包均有解压码：</b> <a rel="nofollow" target="_blank"><b>点击下载压缩包解压工具</b></a></p></div><div class="tit"><h3>图书目录</h3></div><div id="book-contents"><p>Introduction1</p><p>1．What is a Neural Network?1</p><p>2．The Human Brain6</p><p>3．Models of a Neuron10</p><p>4．Neural Networks Viewed As Directed Graphs15</p><p>5．Feedback18</p><p>6．Network Architectures21</p><p>7．Knowledge Representation24</p><p>8．Learning Processes34</p><p>9．Learning Tasks38</p><p>10．Concluding Remarks45</p><p>Notes and References46</p><p>Chapter 1 Rosenblatt's Perceptron47</p><p>1.1．Introduction47</p><p>1.2．Perceptron48</p><p>1.3．The Perceptron Convergence Theorem50</p><p>1.4．Relation Between the Perceptron and Bayes Classifier for a Gaussian Environment55</p><p>1.5．Computer Experiment：Pattern Classification60</p><p>1.6．The Batch Perceptron Algorithm62</p><p>1.7．Summary and Discussion65</p><p>Notes and References66</p><p>Problems66</p><p>Chapter 2 Model Building through Regression68</p><p>2.1 Introduction68</p><p>2.2 Linear Regression Model：Preliminary Considerations69</p><p>2.3 Maximum a Posteriori Estimation of the Parameter Vector71</p><p>2.4 Relationship Between Regularized Least-Squares Estimation and MAP Estimation76</p><p>2.5 Computer Experiment：Pattern Classification77</p><p>2.6 The Minimum-Description-Length Principle79</p><p>2.7 Finite Sample-Size Considerations82</p><p>2.8 The Instrumental-Variables Method86</p><p>2.9 Summary and Discussion88</p><p>Notes and References89</p><p>Problems89</p><p>Chapter 3 The Least-Mean-Square Algorithm91</p><p>3.1 Introduction91</p><p>3.2 Filtering Structure of the LMS Algorithm92</p><p>3.3 Unconstrained Optimization：a Review94</p><p>3.4 The Wiener Filter100</p><p>3.5 The Least-Mean-Square Algorithm102</p><p>3.6 Markov Model Portraying the Deviation of the LMS Algorithm from the Wiener Filter104</p><p>3.7 The Langevin Equation：Characterization of Brownian Motion106</p><p>3.8 Kushner's Direct-Averaging Method107</p><p>3.9 Statistical LMS Learning Theory for Small Learning-Rate Parameter108</p><p>3.10 Computer Experiment Ⅰ：Linear Prediction110</p><p>3.11 Computer Experiment Ⅱ：Pattern Classification112</p><p>3.12 Virtues and Limitations of the LMS Algorithm113</p><p>3.13 Learning-Rate Annealing Schedules115</p><p>3.14 Summary and Discussion117</p><p>Notes and References118</p><p>Problems119</p><p>Chapter 4 Multilayer Perceptrons122</p><p>4.1 Introduction123</p><p>4.2 Some Preliminaries124</p><p>4.3 Batch Learning and On-Line Learning126</p><p>4.4 The Back-Propagation Algorithm129</p><p>4.5 XOR Problem141</p><p>4.6 Heuristics for Making the Back-Propagation Algorithm Perform Better144</p><p>4.7 Computer Experiment：Pattern Classification150</p><p>4.8 Back Propagation and Differentiation153</p><p>4.9 The Hessian and Its Role in On-Line Learning155</p><p>4.10 Optimal Annealing and Adaptive Control of the Learning Rate157</p><p>4.11 Generalization164</p><p>4.12 Approximations of Functions166</p><p>4.13 Cross-Validation171</p><p>4.14 Complexity Regularization and Network Pruning175</p><p>4.15 Virtues and Limitations of Back-Propagation Learning180</p><p>4.16 Supervised Learning Viewed as an Optimization Problem186</p><p>4.17 Convolutional Networks201</p><p>4.18 Nonlinear Filtering203</p><p>4.19 Small-Scale Versus Large-Scale Learning Problems209</p><p>4.20 Summary and Discussion217</p><p>Notes and References219</p><p>Problems221</p><p>Chapter 5 Kernel Methods and Radial-Basis Function Networks230</p><p>5.1 Introduction230</p><p>5.2 Cover's Theorem on the Separability of Patterns231</p><p>5.3 The Interpolation Problem236</p><p>5.4 Radial-Basis-Function Networks239</p><p>5.5 K-Means Clustering242</p><p>5.6 Recursive Least-Squares Estimation of the Weight Vector245</p><p>5.7 Hybrid Learning Procedure for RBF Networks249</p><p>5.8 Computer Experiment：Pattern Classification250</p><p>5.9 Interpretations of the Gaussian Hidden Units252</p><p>5.10 Kernel Regression and Its Relation to RBF Networks255</p><p>5.11 Summary and Discussion259</p><p>Notes and References261</p><p>Problems263</p><p>Chapter 6 Support Vector Machines268</p><p>6.1 Introduction268</p><p>6.2 Optimal Hyperplane for Linearly Separable Patterns269</p><p>6.3 Optimal Hyperplane for Nonseparable Patterns276</p><p>6.4 The Support Vector Machine Viewed as a Kernel Machine281</p><p>6.5 Design of Support Vector Machines284</p><p>6.6 XOR Problem286</p><p>6.7 Computer Experiment：Pattern Classification289</p><p>6.8 Regression：Robustness Considerations289</p><p>6.9 Optimal Solution of the Linear Regression Problem293</p><p>6.10 The Representer Theorem and Related Issues296</p><p>6.11 Summary and Discussion302</p><p>Notes and References304</p><p>Problems307</p><p>Chapter 7 Regularization Theory313</p><p>7.1 Introduction313</p><p>7.2 Hadamard's Conditions for Well-Posedness314</p><p>7.3 Tikhonov's Regularization Theory315</p><p>7.4 Regularization Networks326</p><p>7.5 Generalized Radial-Basis-Function Networks327</p><p>7.6 The Regularized Least-Squares Estimator：Revisited331</p><p>7.7 Additional Notes of Interest on Regularization335</p><p>7.8 Estimation of the Regularization Parameter336</p><p>7.9 Semisupervised Learning342</p><p>7.10 Manifold Regularization：Preliminary Considerations343</p><p>7.11 Differentiable Manifolds345</p><p>7.12 Generalized Regularization Theory348</p><p>7.13 Spectral Graph Theory350</p><p>7.14 Generalized Representer Theorem352</p><p>7.15 Laplacian Regularized Least-Squares Algorithm354</p><p>7.16 Experiments on Pattern Classification Using Semisupervised Learning356</p><p>7.17 Summary and Discussion359</p><p>Notes and References361</p><p>Problems363</p><p>Chapter 8 Principal-Components Analysis367</p><p>8.1 Introduction367</p><p>8.2 Principles of Self-Organization368</p><p>8.3 Self-Organized Feature Analysis372</p><p>8.4 Principal-Components Analysis：Perturbation Theory373</p><p>8.5 Hebbian-Based Maximum Eigenfilter383</p><p>8.6 Hebbian-Based Principal-Components Analysis392</p><p>8.7 Case Study：Image Coding398</p><p>8.8 Kernel Principal-Components Analysis401</p><p>8.9 Basic Issues Involved in the Coding of Natural Images406</p><p>8.10 Kernel Hebbian Algorithm407</p><p>8.11 Summary and Discussion412</p><p>Notes and References415</p><p>Problems418</p><p>Chapter 9 Self-Organizing Maps425</p><p>9.1 Introduction425</p><p>9.2 Two Basic Feature-Mapping Models426</p><p>9.3 Self-Organizing Map428</p><p>9.4 Properties of the Feature Map437</p><p>9.5 Computer Experiments Ⅰ：Disentangling Lattice Dynamics Using SOM445</p><p>9.6 Contextual Maps447</p><p>9.7 Hierarchical Vector Quantization450</p><p>9.8 Kernel Self-Organizing Map454</p><p>9.9 Computer Experiment Ⅱ：Disentangling Lattice Dynamics Using Kernel SOM462</p><p>9.10 Relationship Between Kernel SOM and Kullback-Leibler Divergence464</p><p>9.11 Summary and Discussion466</p><p>Notes and References468</p><p>Problems470</p><p>Chapter 10 Information-Theoretic Learning Models475</p><p>10.1 Introduction476</p><p>10.2 Entropy477</p><p>10.3 Maximum-Entropy Principle481</p><p>10.4 Mutual Information484</p><p>10.5 Kullback-Leibler Divergence486</p><p>10.6 Copulas489</p><p>10.7 Mutual Information as an Objective Function to be Optimized493</p><p>10.8 Maximum Mutual Information Principle494</p><p>10.9 Infomax and Redundancy Reduction499</p><p>10.10 Spatially Coherent Features501</p><p>10.11 Spatially Incoherent Features504</p><p>10.12 Independent-Components Analysis508</p><p>10.13 Sparse Coding of Natural Images and Comparison with ICA Coding514</p><p>10.14 Natural-Gradient Learning for Independent-Components Analysis516</p><p>10.15 Maximum-Likelihood Estimation for Independent-Components Analysis526</p><p>10.16 Maximum-Entropy Learning for Blind Source Separation529</p><p>10.17 Maximization of Negentropy for Independent-Components Analysis534</p><p>10.18 Coherent Independent-Components Analysis541</p><p>10.19 Rate Distortion Theory and Information Bottleneck549</p><p>10.20 Optimal Manifold Representation of Data553</p><p>10.21 Computer Experiment：Pattern Classification560</p><p>10.22 Summary and Discussion561</p><p>Notes and References564</p><p>Problems572</p><p>Chapter 11 Stochastic Methods Rooted in Statistical Mechanics579</p><p>11.1 Introduction580</p><p>11.2 Statistical Mechanics580</p><p>11.3 Markov Chains582</p><p>11.4 Metropolis Algorithm591</p><p>11.5 Simulated Annealing594</p><p>11.6 Gibbs Sampling596</p><p>11.7 Boltzmann Machine598</p><p>11.8 Logistic Belief Nets604</p><p>11.9 Deep Belief Nets606</p><p>11.10 Deterministic Annealing610</p><p>11.11 Analogy of Deterministic Annealing with Expectation-Maximization Algorithm616</p><p>11.12 Summary and Discussion617</p><p>Notes and References619</p><p>Problems621</p><p>Chapter 12 Dynamic Programming627</p><p>12.1 Introduction627</p><p>12.2 Markov Decision Process629</p><p>12.3 Bellman's Optimality Criterion631</p><p>12.4 Policy Iteration635</p><p>12.5 Value Iteration637</p><p>12.6 Approximate Dynamic Programming：Direct Methods642</p><p>12.7 Temporal-Difference Learning643</p><p>12.8 Q-Learning648</p><p>12.9 Approximate Dynamic Programming：Indirect Methods652</p><p>12.10 Least-Squares Policy Evaluation655</p><p>12.11 Approximate Policy Iteration660</p><p>12.12 Summary and Discussion663</p><p>Notes and References665</p><p>Problems668</p><p>Chapter 13 Neurodynamics672</p><p>13.1 Introduction672</p><p>13.2 Dynamic Systems674</p><p>13.3 Stability of Equilibrium States678</p><p>13.4 Attractors684</p><p>13.5 Neurodynamic Models686</p><p>13.6 Manipulation of Attractors as a Recurrent Network Paradigm689</p><p>13.7 Hopfield Model690</p><p>13.8 The Cohen-Grossberg Theorem703</p><p>13.9 Brain-State-In-A-Box Model705</p><p>13.10 Strange Attractors and Chaos711</p><p>13.11 Dynamic Reconstruction of a Chaotic Process716</p><p>13.12 Summary and Discussion722</p><p>Notes and References724</p><p>Problems727</p><p>Chapter 14 Bayseian Filtering for State Estimation of Dynamic Systems731</p><p>14.1 Introduction731</p><p>14.2 State-Space Models732</p><p>14.3 Kalman Filters736</p><p>14.4 The Divergence-Phenomenon and Square-Root Filtering744</p><p>14.5 The Extended Kalman Filter750</p><p>14.6 The Bayesian Filter755</p><p>14.7 Cubature Kalman Filter：Building on the Kalman Filter759</p><p>14.8 Particle Filters765</p><p>14.9 Computer Experiment：Comparative Evaluation of Extended Kalman and Particle Filters775</p><p>14.10 Kalman Filtering in Modeling of Brain Functions777</p><p>14.11 Summary and Discussion780</p><p>Notes and References782</p><p>Problems784</p><p>Chapter 15 Dynamically Driyen Recurrent Networks790</p><p>15.1 Introduction790</p><p>15.2 Recurrent Network Architectures791</p><p>15.3 Universal Approximation Theorem797</p><p>15.4 Controllability and Observability799</p><p>15.5 Computational Power of Recurrent Networks804</p><p>15.6 Learning Algorithms806</p><p>15.7 Back Propagation Through Time808</p><p>15.8 Real-Time Recurrent Learning812</p><p>15.9 Vanishing Gradients in Recurrent Networks818</p><p>15.10 Supervised Training Framework for Recurrent Networks Using Nonlinear Sequential State Estimators822</p><p>15.11 Computer Experiment：Dynamic Reconstruction of Mackay-Glass Attractor829</p><p>15.12 Adaptivity Considerations831</p><p>15.13 Case Study：Model Reference Applied to Neurocontrol833</p><p>15.14 Summary and Discussion835</p><p>Notes and References839</p><p>Problems842</p><p>Bibliography845</p><p>Index889</p><p></p></div></div><div class="d-rt"><h3>热门推荐</h3><ul><li><a href="/book/3600975.html">3600975.html</a></li><li><a href="/book/3776959.html">3776959.html</a></li><li><a href="/book/11632.html">11632.html</a></li><li><a href="/book/781536.html">781536.html</a></li><li><a href="/book/3553.html">3553.html</a></li><li><a href="/book/1536141.html">1536141.html</a></li><li><a href="/book/3403695.html">3403695.html</a></li><li><a href="/book/3030598.html">3030598.html</a></li><li><a href="/book/2585121.html">2585121.html</a></li><li><a href="/book/2848646.html">2848646.html</a></li></ul></div></div><div id="footer"><p>Copyright&nbsp;&copy;&nbsp;2025&nbsp;&nbsp;<a href="/list/">最新更新</a></p><p>请使用FDM BitComet qBittorrent uTorrent等BT下载工具，下载本站电子书资源！首推Free Download Manager下载软件。文件页数>标注页数[分册图书除外]</p></div></body></html>