<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><script type="text/javascript" src="https://file.bkxsj.com/skin/book/js/sk.js"></script><meta name="robots" content="index,follow"><title>Pattern Classification[PDF|Epub|txt|kindle电子书版本网盘下载]-灵感之桥</title><meta name="Keywords" content="Pattern Classification"/><meta name="description" content="Pattern Classificationpdf下载文件大小为50MB,PDF页数为676页"/><meta http-equiv="X-UA-Compatible" content="IE=9; IE=8; IE=7; IE=EDGE;chrome=1"><link type="image/x-icon" rel="shortcut icon" href="https://www.shukui.net/skin/book/images/favicon.ico"><link type="text/css" rel="stylesheet" href="https://www.shukui.net/skin/book/css/style.css"><style>#main .d-main {margin-left: 0;width: 620px;}.down-btn {animation: myShake 2.5s linear .15s infinite}@keyframes myShake {0%, 66% {transform: translateZ(0)}67%, 73.6%, 83.6%, 93.6%, to {animation-timing-function: cubic-bezier(.215, .61, .355, 1);transform: translateZ(0)}80.3%, 81.4% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -4px, 0)}90.3% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -2px, 0)}97% {transform: translate3d(0, -.5px, 0)}}.copylink-btn {margin-right: 20px;}.copymd5-btn {margin-bottom: 25px;margin-left: 10px;}</style></head><body><div id="header"><div class="inner"><div class="logo"><a href="/"><img width="103" height="25" alt="灵感之桥"src="https://www.shukui.net/skin/book/images/logo.png"></a></div><div class="search"><form action="/so/search.php" target="_blank"><input type="text" autocomplete="off" id="bdcsMain" name="q" placeholder="书名 / 作者 / 出版社 / ISBN"class="inp-txt"><select class="inp-select" id="datasource" onchange="selectDatasource(this)"><option value="so">主库</option><option value="s">从库</option></select><input type="submit" value="搜索" class="inp-btn"></form></div></div></div><div id="main"><div class="d-main"><div class="tit"><h3>图书介绍</h3></div><h1 class="book-name">Pattern ClassificationPDF|Epub|txt|kindle电子书版本网盘下载</h1><div class="d-info"><div class="b-thumb"><img src="https://www.shukui.net/cover/35/34089586.jpg" alt="Pattern Classification"></div><div class="b-info"><ul><li>Second Edition 著</li><li>出版社： 机械工业出版社</li><li>ISBN：711113687X</li><li>出版时间：2004</li><li>标注页数：726页</li><li>文件大小：50MB</li><li>文件页数：676页</li><li>主题词：模式分类－英文</li></ul></div></div><div class="tit"><h3>PDF下载</h3></div><div></br><a style="color:red;" rel="external nofollow" href="https://www.kjlm.net/ebook/3041778.html"target="_blank"><b>点此进入-本书在线PDF格式电子书下载【推荐-云解压-方便快捷】直接下载PDF格式图书。移动端-PC端通用</a></b></br><a class="down-btn" rel="external nofollow" href="https://down.trackerbk.com/bt/50/34089586.torrent"target="_blank">种子下载</a>[BT下载速度快]温馨提示：（请使用BT下载软件FDM进行下载）<a rel="nofollow" href="https://www.freedownloadmanager.org/zh/" target="_blank">软件下载地址页</a><a class="down-btn" rel="external nofollow" href="https://down.p2spdb.com/50/34089586.rar" target="_blank">直链下载</a>[便捷但速度慢]&nbsp;&nbsp;<a style="color:red;" rel="external nofollow" href="https://pdfyl.ertongbook.com/70/34089586.pdf" target="_blank"><b>[在线试读本书]</b></a>&nbsp;&nbsp;<b> <a style="color:red;" rel="external nofollow" href="https://web.jyjl.org/index/recovery.html" target="_blank">[在线获取解压码]</a></b><div class="copymd5-btn"><a href="javascript:copyToClip('e7100b7e0c628aafbff2f98c54a9e088')">点击复制MD5值：e7100b7e0c628aafbff2f98c54a9e088</a></div></div><div class="tit"><h3>下载说明</h3></div><div style="margin:20px 10px"><h2>Pattern ClassificationPDF格式电子书版下载</h2>下载的文件为RAR压缩包。需要使用解压软件进行解压得到PDF格式图书。<br><br><div class="copymd5-btn"><a href="javascript:copyToClip('magnet:?xt=urn:btih:RX6G6JQ2LJW7PBBEDNUSCQM7ICDMHIWK')">点击复制85GB完整离线版磁力链接到迅雷FDM等BT下载工具进行下载</a>&nbsp;&nbsp;<a rel="nofollow" target="_blank">详情点击-查看共享计划</a></div>建议使用BT下载工具Free Download Manager进行下载,简称FDM(免费,没有广告,支持多平台）。本站资源全部打包为BT种子。所以需要使用专业的BT下载软件进行下载。如BitComet qBittorrent uTorrent等BT下载工具。迅雷目前由于本站不是热门资源。不推荐使用！后期资源热门了。安装了迅雷也可以迅雷进行下载！<br><br><b>（文件页数 要大于 标注页数，上中下等多册电子书除外）</b><br><br><p style="color:red;"> <b>注意：本站所有压缩包均有解压码：</b> <a rel="nofollow" target="_blank"><b>点击下载压缩包解压工具</b></a></p></div><div class="tit"><h3>图书目录</h3></div><div id="book-contents"><p>1 INTRODUCTION1</p><p>1.1Machine Perception,1</p><p>1.2An Example,1</p><p>1.2.1 Related Fields,8</p><p>1.3Pattern Recognition Systems,9</p><p>1.3.3 Sensing,9</p><p>1.3.2 Segmentation d Grouping,9</p><p>1.3.3 Feature Extraction,11</p><p>1.3.4 Classication,12</p><p>1.3.5 Post Processing,13</p><p>1.4The Design Cycle,14</p><p>1.4.1 Data Collection,14</p><p>1.4.2 Feature Choice,14</p><p>1.4.3 Model Choice,15</p><p>1.4.4 Training,15</p><p>1.4.5 Evaluation,15</p><p>1.4.6 Computational Complexity,16</p><p>1.5Learning and Adaptation,16</p><p>1.5.1 Supervised Learning,16</p><p>1.5.2 Unsupervised Learning,17</p><p>1.5.3 Reinforcement Learning,17</p><p>1.6 Conclusion,17</p><p>Summa by Chapters,17</p><p>Bibliographical and Historical Remarks,18</p><p>Bibliography,19</p><p>2 BAYESIAN DECISION THEORY20</p><p>2.1 Introduction,20</p><p>2.2 Bayesi Decision Theo—Continuous Features,24</p><p>2.2.1 Two-Catego Classication,25</p><p>2.3 Minimum-Error-Rate Classication,26</p><p>2.3.1 Minimax Criterion,27</p><p>2.3.2 Neyman-Pearson Criterion,28</p><p>2.4 Classiers, Discriminant Functions, and Decision Surfaces,29</p><p>2.4.1 The Multicatego Case,29</p><p>2.4.2 The Two-Catego Case,30</p><p>2.5 The Normal Density,31</p><p>2.5.1 Univaate Density,32</p><p>2.5.2 Multivariate Density,33</p><p>2.6 Discminant Functions for the Normal Density,36</p><p>2.6.1 Case 1: Mi =o2I,36</p><p>2.6.2 Case 2: Mi = M,39</p><p>2.6.3 Case 3: Mi = arbitrary,41</p><p>Example 1 Decision Regions for Two-DimensionalGaussian Data,41</p><p>2.7 Error Probabilities and Integrals,45</p><p>2.8 Error Bounds for Normal Densities,46</p><p>2.8.1 Cheoff Bound,46</p><p>2.8.2 Bhattachaya Bound,47</p><p>Example 2 Error Bounds for Gaussian Distributions,48</p><p>2.8.3 Signal Detection Theo and Operating Charactestics,48</p><p>2.9 Bayes Decision Theo—Discrete Features,51</p><p>2.9.1 Independent Bina Features,52</p><p>Example 3 Bayesian Decisions for Three-DimensionalBina Data,53</p><p>2.10 Missing and Noisy Features,54</p><p>2.10.1 Missing Features,54</p><p>2.10.2 Noisy Features,55</p><p>2.11 Bayesian Belief Networks,56</p><p>Example 4 Belief Network for Fish,59</p><p>2.12 Compound Bayesian Decision Theo and Context,62</p><p>Summa,63</p><p>Bibliographical and Histocal Remarks,64</p><p>Problems,65</p><p>Computer exercises,80</p><p>Bibliography,82</p><p>MAXIMUM-LIKELIHOOD AND BAYESIAN3 PARAMETER ESTIMATION84</p><p>3.1 Introduction,84</p><p>3.2 Maximum-Likelihood Estimation,85</p><p>3.2.1 The General Principle,85</p><p>3.2.2 The Gaussian Case: Unknown u,88</p><p>3.2.3 The Gaussian Case: Unknown ur and ,88</p><p>3.2.4 Bias,89</p><p>3.3 Bayesian Estimation,90</p><p>3.3.1 The Class-Conditional Densities,91</p><p>3.3.2 The Parameter Distribution,91</p><p>3.4 Bayesian Parameter Estimation: Gaussian Case,92</p><p>3.4.1 The Univariate Case: p(u D),92</p><p>3.4.2 The UnivariateCase: p(xD),95</p><p>3.4.3 The Multivaate Case,95</p><p>3.5 Bayesian Parameter Estimation: General Theo,97</p><p>Example 1 Recursive Bayes Learning,98</p><p>3.5.1 When Do Maximum-Likelihood and Bayes Methods Differ?,100</p><p>3.5.2 Noninformative Priors and Invariance,101</p><p>3.5.3 Gibbs Algorithm,102</p><p>3.6 Sufcient Statistics,102</p><p>3.6.1 Sufcient Statistics and the Exponential Family,106</p><p>3.7 Problems of Dimensionality,107</p><p>3.7.1 Accuracy, Dimension, and Training Sample Size,107</p><p>3.7.2 Computational Complexity,111</p><p>3.7.3 Overtting,113</p><p>3.8 Component Analysis and Discriminants,114</p><p>3.8.1 Principal Component Analysis (PCA),115</p><p>3.8.2 Fisher Linear Discriminant,117</p><p>3.8.3 Multiple Discriminant Analysis,121</p><p>3.9 Expectation-Maximization (EM),124</p><p>Example 2 Expectation-Maximization for a 21D Normal Model,126</p><p>3.10 Hidden Markov Models,128</p><p>3.10.1 First-Order Markov Models,128</p><p>3.10.2 First-Order Hidden Markov Models,129</p><p>3.10.3 Hidden Markov Model Computation,129</p><p>3.10.4 Evaluation,131</p><p>Example 3 Hidden Markov Model,133</p><p>3.10.5 Decoding,135</p><p>Example 4 HMM Decoding,136</p><p>3.10.6 Learning,137</p><p>Summa,139</p><p>Bibliographical and Historical Remarks,139</p><p>Problems,140</p><p>Computer exercises,155</p><p>Bibliography,159</p><p>4 NONPARAMETRIC TECHNIQUES161</p><p>4.1 Introduction,161</p><p>4.2 Density Estimation,161</p><p>4.3 Parzen Windows,164</p><p>4.3.1 Convergence of the Mean,167</p><p>4.3.2 Convergence of the Variance,167</p><p>4.3.3 Illustrations,168</p><p>4.3.4 Classication Example,168</p><p>4.3.5 Probabilistic Neural Networks (PNNS),172</p><p>4.3.6 Choosing the Window Function,174</p><p>4.4 k-Nearest-Neighbor Estimation,174</p><p>4.4.1 kn-Neast-Neighbor and Parzen-Window Estimation,176</p><p>4.4.2 Estimation of A Posteriori Probabilities,177</p><p>4.5 The Nearest-Neighbor Rule,177</p><p>4.5.1 Convergence of the Nearest Neighbor,179</p><p>4.5.2 Error Rate for the Nearest-Neighbor Rule,180</p><p>4.5.3 Error Bounds,180</p><p>4.5.4 The k-Nearest-Neighbor Rule,182</p><p>4.5.5 Computational Complexity of the k-Nearest-Neighbor Rule,184</p><p>4.6 Metrics and Nearest-Neighbor Classication,187</p><p>4.6.1 Properties of Metrics,187</p><p>4.6.2 Tangent Distance,188</p><p>4.7 Fuzzy Classication,192</p><p>4.8 Reduced Coulomb Energy Networks,195</p><p>4.9 Approximations by Series Expansions,197</p><p>Summa,199</p><p>Bibliographical and Historical Remarks,200</p><p>Problems,201</p><p>Computer exercises,209</p><p>Bibliography,213</p><p>5 LINEAR DISCRIMINANT FUNCTIONS215</p><p>5.1 Introduction,215</p><p>5.2 Linear Discriminant Functions and Decision Surfaces,216</p><p>5.2.1 The Two-Catego Case,216</p><p>5.2.2 The Multicatego Case,218</p><p>5.3 Generalized Linear Discriminant Functions,219</p><p>5.4 The Two-Catego Linearly Separable Case,223</p><p>5.4.1 Geomet and Terminology,224</p><p>5.4.2 Gradient Descent Procedures,224</p><p>5.5 Minimizing the Perceptron Criterion Function,227</p><p>5.5.1 The Perceptron Criterion Function,227</p><p>5.5.2 Convergence Proof for Single-Sample Correction,229</p><p>5.5.3 Some Direct Generalizations,232</p><p>5.6 Relaxation Procedures,235</p><p>5.6.1 The Descent Algorithm,235</p><p>5.6.2 Convergence Proof,237</p><p>5.7 Nonseparable Behavior,238</p><p>5.8 Minimum Squared-Error Procedures,239</p><p>5.8.1 Minimum Squared-Error and the Pseudoinverse,240</p><p>Example 1 Constructing a Linear Classier by MatrixPseudoinverse,241</p><p>5.8.2 Relation to Fisher's Linear Discriminant,242</p><p>5.8.3 Asymptotic Approximation to an Optimal Discriminant,243</p><p>5.8.4 The Widrow-Ho or LMS Procedure,245</p><p>5.8.5 Stochastic Approximation Methods,246</p><p>5.9 The Ho-Kashyap Procedures,249</p><p>5.9.1 The Descent Procedure,250</p><p>5.9.2 Convergence Proof,251</p><p>5.9.3 Nonseparable Behavior,253</p><p>5.9.4 Some Related Procedures,253</p><p>5.10 Linear Programming Algorithms,256</p><p>5.10.1 Linear Programming,256</p><p>5.10.2 The Linearly Separable Case,257</p><p>5.10.3 Minimizing the Perceptron Criterion Function,258</p><p>5.11 Suppo Vector Machines,259</p><p>5.11.1 SVM Training,263</p><p>Example 2 SVM for the XOR Problem,264</p><p>5.12 Multicatego Generalizations,265</p><p>5.12.1 Kesler's Construction266</p><p>5.12.2 Convergence of the Fixed-Increment Rule,266</p><p>5.12.3 Generalizations for MSE Procedures,268</p><p>Summa,269</p><p>Bibliographical and Historical Remarks,270</p><p>Problems,271</p><p>Computer exercises,278</p><p>Bibliography,281</p><p>6 MULTILAYER NEURAL NETWORKS282</p><p>6.1 Introduction,282</p><p>6.2 Feedforward Operation and Classication,284</p><p>6.2.1 General Feedforward Operation,286</p><p>6.2.2 Expressive Power of Multilayer Networks,287</p><p>6.3 Backpropagation Algorithm,288</p><p>6.3.1 Network Leaing,289</p><p>6.3.2 Training Protocols,293</p><p>6.3.3 Leaing Curves,295</p><p>6.4 Error Surfaces,296</p><p>6.4.1 Some Small Networks,296</p><p>6.4.2 The Exclusive-OR (XOR),298</p><p>6.4.3 Larger Networks,298</p><p>6.4.4 How Important Are Multiple Minima?,299</p><p>6.5 Backpropagation as Feature Mapping,299</p><p>6.5.1 Representations at the Hidden Layer—Weights,302</p><p>6.6 Backpropagation, Bayes Theo and Probability,303</p><p>6.6.1 Bayes Discriminants and Neural Networks,303</p><p>6.6.2 Outputs as Probabilities,304</p><p>6.7 Related Statistical Techniques,305</p><p>6.8 Practical Techniques for Improving Backpropagation,306</p><p>6.8.1 Activation Function,307</p><p>6.8.2 Parameters for the Sigmoid,308</p><p>6.8.3 Scaling Input,308</p><p>6.8.4 Target Values,309</p><p>6.8.5 Training with Noise,310</p><p>6.8.6 Manufacturing Data,310</p><p>6.8.7 Number of Hidden Units,310</p><p>6.8.8 Initializing Weights,311</p><p>6.8.9 Leaming Rates,312</p><p>6.8.10 Momentum,313</p><p>6.8.11 Weight Decay,314</p><p>6.8.12 Hints,315</p><p>6.8.13 On-Line, Stochastic or Batch Training?,316</p><p>6.8.14 Stopped Training,316</p><p>6.8.15 Number of Hidden Layers,317</p><p>6.8.16 Criterion Function,318</p><p>6.9 Second-Order Methods,318</p><p>6.9.1 Hessian Matrix,318</p><p>6.9.2 Newton's Method,319</p><p>6.9.3 Quickprop,320</p><p>6.9.4 Conjugate Gradient Descent,321</p><p>Example 1 Conjugate Gradient Descent,322</p><p>6.10 Additional Networks and Training Methods,324</p><p>6.10.1 Radial Basis Function Networks (RBFs),324</p><p>6.10.2 Special Bases,325</p><p>6.10.3 Matched Filters,325</p><p>6.10.4 Convolutional Networks,326</p><p>6.10.5 Recurrent Networks,328</p><p>6.10.6 Cascade-Correlation,329</p><p>6.11 Regularization, Complexity Adjustment and Pruning,330</p><p>Summa,333</p><p>Bibliographical and Historical Remarks,333</p><p>Problems,335</p><p>Computer exercises,343</p><p>Bibliography,347</p><p>7 STOCHASTIC METHODS350</p><p>7.1 Introduction,350</p><p>7.2 Stochastic Search,351</p><p>7.2.1 Simulated Annealing,351</p><p>7.2.2 The Boltzmann Factor,352</p><p>7.2.3 Deterministic Simulated Annealing,357</p><p>7.3 Boltzmann Learning,360</p><p>7.3.1 Stochastic Boltzmann Learning of Visible States,360</p><p>7.3.2 Missing Features and Catego Constraints,365</p><p>7.3.3 Deterministic Boltzmann Learning,366</p><p>7.3.4 Initialization and Setting Parameters,367</p><p>7.4 Boltzmann Networks and Graphical Models,370</p><p>7.4.1 Other Graphical Models,372</p><p>7.5 Evolutiona Methods,373</p><p>7.5.1 Genetic Algorithms,373</p><p>7.5.2 Further Heuristics,377</p><p>7.5.3 Why Do They Work?,378</p><p>7.6 Genetic Programming,378</p><p>Summa,381</p><p>Bibliographical and Historical Remarks,381</p><p>Problems,383</p><p>Computer exercises,388</p><p>Bibliography,391</p><p>8 NONMETRIC METHODS394</p><p>8.1 Introduction,394</p><p>8.2 Decision Trees,395</p><p>8.3 CART,396</p><p>8.3.1 Number of Splits,397</p><p>8.3.2 Que Selection and Node Impurity,398</p><p>8.3.3 When to Stop Spliing,402</p><p>8.3.4 Pruning,403</p><p>8.3.5 Assignment of Leaf Node Labels,404</p><p>Example 1 A Simple Tree,404</p><p>8.3.6 Computational Complexi,406</p><p>8.3.7 Featu Choice,407</p><p>8.3.8 Multivariate Decision Trees,408</p><p>8.3.9 Priors and Costs,409</p><p>8.3.10 Missing Attributes,409</p><p>Example 2 Surrogate Splits and Missing Attributes,410</p><p>8.4 Other Tree Methods,411</p><p>8.4.1 ID3,411</p><p>8.4.2 C4.5,411</p><p>8.4.3 Which Te Classier Is Best?,412</p><p>8.5 Recognition with Strings,413</p><p>8.5.1 String Matching,415</p><p>8.5.2 Edit Distance,418</p><p>8.5.3 Computational Complexity,420</p><p>8.5.4 String Matching with Errors,420</p><p>8.5.5 String Matching with the “Don't-Ca” Symbol,421</p><p>8.6 Grammatical Methods,421</p><p>8.6.1 Grammars,422</p><p>8.6.2 pes of String Grammars,424</p><p>Example 3 A Grammar for Pronouncing Numbers,425</p><p>8.6.3 Recognition Using Grammars,426</p><p>8.7 Grammatical Inference,429</p><p>Example 4 Grammatical Inference,431</p><p>8.8 Rule-Based Methods,431</p><p>8.8.1 Learning Rules,433</p><p>Summa,434</p><p>Bibliographical and Historical Remarks,435</p><p>Problems,437</p><p>Computer exercises,446</p><p>Bibliography,450</p><p>9 ALGORITHM-INDEPENDENT MACHINE LEARNING453</p><p>9.1 Introduction,453</p><p>9.2 Lack of Inhent Superiority of Any Classier,454</p><p>9.2.1 No Fe Lunch Theorem,454</p><p>Example 1 No Free Lunch for Bina Data,457</p><p>9.2.2 Ugly Duckling Theorem,458</p><p>9.2.3 Minimum Description Length (MDL),461</p><p>9.2.4 Minimum Description Length Principle,463</p><p>9.2.5 Overing Avoidance and Occam's Razor,464</p><p>9.3 Bias and Variance,465</p><p>9.3.1 Bias and Variance for Regression,466</p><p>9.3.2 Bias and Variance for Classication,468</p><p>9.4 Resampling for Estimating Statistics,471</p><p>9.4.1 Jackknife,472</p><p>Example 2 Jackknife Estimate of Bias and Variance of the Mode,473</p><p>9.4.2 Bootstrap,474</p><p>9.5 Resampling for Classier Design,475</p><p>9.5.1 Bagging,475</p><p>9.5.2 Boosting,476</p><p>9.5.3 Learning with Queries,480</p><p>9.5.4 Arcing, Leaing with Queries, Bias and Variance,482</p><p>9.6 Estimating and Comparing Classiers,482</p><p>9.6.1 Parametric Models,483</p><p>9.6.2 Cross-Validation,483</p><p>9.6.3 Jackknife and Bootstrap Estimation of Classication Accuracy,485</p><p>9.6.4 Maximum-Likelihood Model Comparison,486</p><p>9.6.5 Bayesian Model Comparison,487</p><p>9.6.6 The Problem-Average Error Rate,489</p><p>9.6.7 Predicting Final Performance om Learning Curves,492</p><p>9.6.8 The Capacity of a Separating Plane,494</p><p>9.7 Combining Classiers,495</p><p>9.7.1 Component Classiers with Discriminant Functions,496</p><p>9.7.2 Component Classiers without Discriminant Functions,498</p><p>Summa,499</p><p>Bibliographical and Historical Remarks,500</p><p>Problems,502</p><p>Computer exercises,508</p><p>Bibliography,513</p><p>10 UNSUPERVISED LEARNING AND CLUSTERING517</p><p>10.1 Introduction,517</p><p>10.2 Mixture Densities and Identiability,518</p><p>10.3 Maximum-Likelihood Estimates,519</p><p>10.4 Application to Normal Mixtures,521</p><p>10.4.1 Case 1: Unknown Mean Vectors,522</p><p>10.4.2 Case 2: All Parameters Unknown,524</p><p>10.4.3 k-Means Clustering,526</p><p>10.4.4 Fuzzy k-Means Clustering,528</p><p>10.5 Unsupervised Bayesian Leaing,530</p><p>10.5.1 The Bayes Classier,530</p><p>10.5.2 Leaing the Parameter Vector,531</p><p>Example 1 Unsupeised Learning of Gaussian Data,534</p><p>10.5.3 Decision-Directed Approximation,536</p><p>10.6 Data Description and Clustering,537</p><p>10.6.1 Similarity Measures,538</p><p>10.7 Criterion Functions for Clustering,542</p><p>10.7.1 The Sum-of-Squared-Error Criterion,542</p><p>10.7.2 Related Minimum Variance Criteria,543</p><p>10.7.3 Scaer Criteria,544</p><p>Example 2 Clustering Criteria,546</p><p>10.8 Iterative Optimization,548</p><p>10.9 Hierarchical Clustering,550</p><p>10.9.1 Denitions,551</p><p>10.9.2 Agglomerative Hierarchical Clustering,552</p><p>10.9.3 Stepwise-Optimal Hierarchical Clustering,555</p><p>10.9.4 Hierarchical Clustering and Induced Metrics,556</p><p>10.10 The Problem of Validity,557</p><p>10.11 On-line clustering,559</p><p>10.11.1 Unknown Number of Clusters,561</p><p>10.11.2 Adaptive Resonance,563</p><p>10.11.3 Leaing with a Critic,565</p><p>10.12 Graph-Theoretic Methods,566</p><p>10.13 Component Analysis,568</p><p>10.13.1 Principal Component Analysis (PCA),568</p><p>10.13.2 Nonlinear Component Analysis (NLCA),569</p><p>10.13.3 Independent Component Analysis (ICA),570</p><p>10.14 Low-Dimensional Representations and Multidimensional Scaling(MDS),573</p><p>10.14.1 Self-Organizing Feature Maps,576</p><p>10.14.2 Clustering and Dimensionality Reduction,580</p><p>Summa,581</p><p>Bibliographical and Historical Remarks,582</p><p>Problems,583</p><p>Computer exercises,593</p><p>Bibliography,598</p><p>A MATHEMATICAL FOUNDATIONS601</p><p>A.1 Notation,601</p><p>A.2 Linear Algebra,604</p><p>A.2.1 Notation and Preliminaries,604</p><p>A.2.2 Inner Product,605</p><p>A.2.3 Outer Product,606</p><p>A.2.4 Derivaves of Matrices,606</p><p>A.2.5 Determinant and Trace,608</p><p>A.2.6 Matrix Inversion,609</p><p>A.2.7 Eigenvectors and Eigenvalues,609</p><p>A.3 Lagrange Optimization,610</p><p>A.4 Probability Theo,611</p><p>A.4.1 Discrete Random Variables,611</p><p>A.4.2 Expected Values,611</p><p>A.4.3 Pairs of Discrete Random Variables,612</p><p>A.4.4 Statistical Independence,613</p><p>A.4.5 Expected Values of Functions of Two Variables,613</p><p>A.4.6 Conditional Probability,614</p><p>A.4.7 The Law of Total Probability and Bayes Rule,615</p><p>A.4.8 Vector Random Variables,616</p><p>A.4.9 Expectations, Mean Vectors and Covariance Matrices,617</p><p>A.4.10 Continuous Random Variables,618</p><p>A.4.11 Distribuons of Sums of Independent Random Variables,620</p><p>A.4.12 Normal Distributions,621</p><p>A.5 Gaussian Derivatives and Integrals,623</p><p>A.5.1 Multivariate Normal Densities,624</p><p>A.5.2 Bivariate Normal Densities,626</p><p>A.6 Hypothesis Testing,628</p><p>A.6.1 Chi-Squared Test,629</p><p>A.7 Information Theo,630</p><p>A.7.1 Entropy and Information,630</p><p>A.7.2 Relative Entropy,632</p><p>A.7.3 Mutual Information,632</p><p>A.S Computational Complexity,633</p><p>Bibliography,635</p><p>INDEX637</p><p></p></div></div><div class="d-rt"><h3>热门推荐</h3><ul><li><a href="/book/2539282.html">2539282.html</a></li><li><a href="/book/832194.html">832194.html</a></li><li><a href="/book/7525.html">7525.html</a></li><li><a href="/book/2991590.html">2991590.html</a></li><li><a href="/book/2337553.html">2337553.html</a></li><li><a href="/book/3317950.html">3317950.html</a></li><li><a href="/book/2250323.html">2250323.html</a></li><li><a href="/book/1667269.html">1667269.html</a></li><li><a href="/book/2895170.html">2895170.html</a></li><li><a href="/book/2631583.html">2631583.html</a></li></ul></div></div><div id="footer"><p>Copyright&nbsp;&copy;&nbsp;2025&nbsp;&nbsp;<a href="/list/">最新更新</a></p><p>请使用FDM BitComet qBittorrent uTorrent等BT下载工具，下载本站电子书资源！首推Free Download Manager下载软件。文件页数>标注页数[分册图书除外]</p></div></body></html>