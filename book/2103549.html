<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8" /><script type="text/javascript" src="https://file.bkxsj.com/skin/book/js/sk.js"></script><meta name="robots" content="index,follow"><title>并行程序设计原理（英文版）[PDF|Epub|txt|kindle电子书版本网盘下载]-灵感之桥</title><meta name="Keywords" content="并行程序设计原理（英文版）"/><meta name="description" content="并行程序设计原理（英文版）pdf下载文件大小为36MB,PDF页数为357页"/><meta http-equiv="X-UA-Compatible" content="IE=9; IE=8; IE=7; IE=EDGE;chrome=1"><link type="image/x-icon" rel="shortcut icon" href="https://www.shukui.net/skin/book/images/favicon.ico"><link type="text/css" rel="stylesheet" href="https://www.shukui.net/skin/book/css/style.css"><style>#main .d-main {margin-left: 0;width: 620px;}.down-btn {animation: myShake 2.5s linear .15s infinite}@keyframes myShake {0%, 66% {transform: translateZ(0)}67%, 73.6%, 83.6%, 93.6%, to {animation-timing-function: cubic-bezier(.215, .61, .355, 1);transform: translateZ(0)}80.3%, 81.4% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -4px, 0)}90.3% {animation-timing-function: cubic-bezier(.755, .05, .855, .06);transform: translate3d(0, -2px, 0)}97% {transform: translate3d(0, -.5px, 0)}}.copylink-btn {margin-right: 20px;}.copymd5-btn {margin-bottom: 25px;margin-left: 10px;}</style></head><body><div id="header"><div class="inner"><div class="logo"><a href="/"><img width="103" height="25" alt="灵感之桥"src="https://www.shukui.net/skin/book/images/logo.png"></a></div><div class="search"><form action="/so/search.php" target="_blank"><input type="text" autocomplete="off" id="bdcsMain" name="q" placeholder="书名 / 作者 / 出版社 / ISBN"class="inp-txt"><select class="inp-select" id="datasource" onchange="selectDatasource(this)"><option value="so">主库</option><option value="s">从库</option></select><input type="submit" value="搜索" class="inp-btn"></form></div></div></div><div id="main"><div class="d-main"><div class="tit"><h3>图书介绍</h3></div><h1 class="book-name">并行程序设计原理（英文版）PDF|Epub|txt|kindle电子书版本网盘下载</h1><div class="d-info"><div class="b-thumb"><img src="https://www.shukui.net/cover/62/32789611.jpg" alt="并行程序设计原理（英文版）"></div><div class="b-info"><ul><li> 著</li><li>出版社： </li><li>ISBN：</li><li>出版时间：未知</li><li>标注页数：0页</li><li>文件大小：36MB</li><li>文件页数：357页</li><li>主题词：</li></ul></div></div><div class="tit"><h3>PDF下载</h3></div><div></br><a style="color:red;" rel="external nofollow" href="https://www.kjlm.net/ebook/2103552.html"target="_blank"><b>点此进入-本书在线PDF格式电子书下载【推荐-云解压-方便快捷】直接下载PDF格式图书。移动端-PC端通用</a></b></br><a class="down-btn" rel="external nofollow" href="https://down.trackerbk.com/bt/38/32789611.torrent"target="_blank">种子下载</a>[BT下载速度快]温馨提示：（请使用BT下载软件FDM进行下载）<a rel="nofollow" href="https://www.freedownloadmanager.org/zh/" target="_blank">软件下载地址页</a><a class="down-btn" rel="external nofollow" href="https://down.p2spdb.com/38/32789611.rar" target="_blank">直链下载</a>[便捷但速度慢]&nbsp;&nbsp;<a style="color:red;" rel="external nofollow" href="https://pdfyl.ertongbook.com/49/32789611.pdf" target="_blank"><b>[在线试读本书]</b></a>&nbsp;&nbsp;<b> <a style="color:red;" rel="external nofollow" href="https://web.jyjl.org/index/recovery.html" target="_blank">[在线获取解压码]</a></b><div class="copymd5-btn"><a href="javascript:copyToClip('5c1f326b60b6172bfb7b0cf61afcbece')">点击复制MD5值：5c1f326b60b6172bfb7b0cf61afcbece</a></div></div><div class="tit"><h3>下载说明</h3></div><div style="margin:20px 10px"><h2>并行程序设计原理（英文版）PDF格式电子书版下载</h2>下载的文件为RAR压缩包。需要使用解压软件进行解压得到PDF格式图书。<br><br><div class="copymd5-btn"><a href="javascript:copyToClip('magnet:?xt=urn:btih:RX6G6JQ2LJW7PBBEDNUSCQM7ICDMHIWK')">点击复制85GB完整离线版磁力链接到迅雷FDM等BT下载工具进行下载</a>&nbsp;&nbsp;<a rel="nofollow" target="_blank">详情点击-查看共享计划</a></div>建议使用BT下载工具Free Download Manager进行下载,简称FDM(免费,没有广告,支持多平台）。本站资源全部打包为BT种子。所以需要使用专业的BT下载软件进行下载。如BitComet qBittorrent uTorrent等BT下载工具。迅雷目前由于本站不是热门资源。不推荐使用！后期资源热门了。安装了迅雷也可以迅雷进行下载！<br><br><b>（文件页数 要大于 标注页数，上中下等多册电子书除外）</b><br><br><p style="color:red;"> <b>注意：本站所有压缩包均有解压码：</b> <a rel="nofollow" target="_blank"><b>点击下载压缩包解压工具</b></a></p></div><div class="tit"><h3>图书目录</h3></div><div id="book-contents"><p>PART 1 Foundations1</p><p>Chapter 1 Introduction1</p><p>The Power and Potential of Parallelism2</p><p>Parallelism,a Familiar Concept2</p><p>Parallelism in Computer Programs3</p><p>Multi-Core Computers,an Opportunity4</p><p>Even More Opportunities to Use Parallel Hardware5</p><p>Parallel Computing versus Distributed Computing6</p><p>System Level Parallelism6</p><p>Convenience of Parallel Abstractions8</p><p>Examining Sequential and Parallel Programs8</p><p>Parallelizing Compilers8</p><p>A Paradigm Shift9</p><p>Parallel Prefix Sum13</p><p>Parallelism Using Multiple Instruction Streams15</p><p>The Concept of a Thread15</p><p>A Multithreaded Solution to Counting 3s15</p><p>The Goals：Scalability and Performance Portability25</p><p>Scalability25</p><p>Performance Portability26</p><p>Principles First27</p><p>Chapter Summary27</p><p>Historical Perspective28</p><p>Exercises28</p><p>Chapter 2 Understanding Parallel Computers30</p><p>Balancing Machine Specifics with Portability30</p><p>A Look at Six Parallel Computers31</p><p>Chip Multiprocessors31</p><p>Symmetric Multiprocessor Architectures34</p><p>Heterogeneous Chip Designs36</p><p>Clusters39</p><p>Supercomputers40</p><p>Observations from Our Six Parallel Computers43</p><p>An Abstraction of a Sequential Computer44</p><p>Applying the RAM Model44</p><p>Evaluating the RAM Model45</p><p>The PRAM：A Parallel Computer Model46</p><p>The CTA：A Practical Parallel Computer Model47</p><p>The CTA Model47</p><p>Communication Latency49</p><p>Properties of the CTA52</p><p>Memory Reference Mechanisms53</p><p>Shared Memory53</p><p>One-Sided Communication54</p><p>Message Passing54</p><p>Memory Consistency Models55</p><p>Programming Models56</p><p>A Closer Look at Communication57</p><p>Applying the CTA Model58</p><p>Chapter Summary59</p><p>Historical Perspective59</p><p>Exercises59</p><p>Chapter 3 Reasoning about Performance61</p><p>Motivation and Basic Concepts61</p><p>Parallelism versus Performance61</p><p>Threads and Processes62</p><p>Latency and Throughput62</p><p>Sources of Performance Loss64</p><p>Overhead64</p><p>Non-Parallelizable Code65</p><p>Contention67</p><p>Idle Time67</p><p>Parallel Structure68</p><p>Dependences68</p><p>Dependences Limit Parallelism70</p><p>Granularity72</p><p>Locality73</p><p>Performance Trade-Offs73</p><p>Communication versus Computation74</p><p>Memory versus Parallelism75</p><p>Overhead versus Parallelism75</p><p>Measuring Performance77</p><p>Execution Time77</p><p>Speedup78</p><p>Superlinear Speedup78</p><p>Efficiency79</p><p>Concerns with Speedup79</p><p>Scaled Speedup versus Fixed-Size Speedup81</p><p>Scalable Performance81</p><p>Scalable Performance Is Difficult to Achieve81</p><p>Implications for Hardware82</p><p>Implications for Software83</p><p>Scaling the Problem Size83</p><p>Chapter Summary84</p><p>Historical Perspective84</p><p>Exercises85</p><p>PART 2 Parallel Abstractions87</p><p>Chapter 4 First Steps Toward Parallel Programming88</p><p>Data and Task Parallelism88</p><p>Definitions88</p><p>Illustrating Data and Task Parallelism89</p><p>The Peril-L Notation89</p><p>Extending C90</p><p>Parallel Threads90</p><p>Synchronization and Coordination91</p><p>Memory Model92</p><p>Synchronized Memory94</p><p>Reduce and Scan95</p><p>The Reduce Abstraction96</p><p>Count 3s Example97</p><p>Formulating Parallelism97</p><p>Fixed Parallelism97</p><p>Unlimited Parallelism98</p><p>Scalable Parallelism99</p><p>Alphabetizing Example100</p><p>Unlimited Parallelism101</p><p>Fixed Parallelism102</p><p>Scalable Parallelism104</p><p>Comparing the Three Solutions109</p><p>Chapter Summary110</p><p>Historical Perspective110</p><p>Exercises110</p><p>Chapter 5 Scalable Algorithmic Techniques112</p><p>Blocks of Independent Computation112</p><p>Schwartz'Algorithm113</p><p>The Reduce and Scan Abstractions115</p><p>Example of Generalized Reduces and Scans116</p><p>The Basic Structure118</p><p>Structure for Generalized Reduce119</p><p>Example of Components of a Generalized Scan122</p><p>Applying the Generalized Scan124</p><p>Generalized Vector Operations125</p><p>Assigning Work to Processes Statically125</p><p>Block Allocations126</p><p>Overlap Regions128</p><p>Cyclic and Block Cyclic Allocations129</p><p>Irregular Allocations132</p><p>Assigning Work to Processes Dynamically134</p><p>Work Queues134</p><p>Variations of Work Queues137</p><p>Case Study：Concurrent Memory Allocation137</p><p>Trees139</p><p>Allocation by Sub-Tree139</p><p>Dynamic Allocations140</p><p>Chapter Summary141</p><p>Historical Perspective142</p><p>Exercises142</p><p>PART 3 Parallel Programming Languages143</p><p>Chapter 6 Programming with Threads145</p><p>POSIX Threads145</p><p>Thread Creation and Destruction146</p><p>Mutual Exclusion150</p><p>Synchronization153</p><p>Safety Issues163</p><p>Performance Issues167</p><p>Case Study：Successive Over-Relaxation174</p><p>Case Study：Overlapping Synchronization with Computation179</p><p>Case Study：Streaming Computations on a Multi-Core Chip187</p><p>Java Threads187</p><p>Synchronized Methods189</p><p>Synchronized Statements189</p><p>The Count 3s Example190</p><p>Volatile Memory192</p><p>Atomic Objects192</p><p>Lock Objects193</p><p>Executors193</p><p>Concurrent Collections193</p><p>OpenMP193</p><p>The Count 3s Example194</p><p>Semantic Limitations on parallel for195</p><p>Reduction196</p><p>Thread Behavior and Interaction197</p><p>Sections199</p><p>Summary of OpenMP199</p><p>Chapter Summary200</p><p>Historical Perspective200</p><p>Exercises200</p><p>Chapter 7 MPI and Other Local View Languages202</p><p>MPI：The Message Passing Interface202</p><p>The Count 3s Example203</p><p>Groups and Communicators211</p><p>Point-to-Point Communication212</p><p>Collective Communication214</p><p>Example：Successive Over-Relaxation219</p><p>Performance Issues222</p><p>Safety Issues228</p><p>Partitioned Global Address Space Languages229</p><p>Co-Array Fortran230</p><p>Unified Parallel C231</p><p>Titanium232</p><p>Chapter Summary233</p><p>Historical Perspective234</p><p>Exercises234</p><p>Chapter 8 ZPL and Other Global View Languages236</p><p>The ZPL Programming Language236</p><p>Basic Concepts of ZPL237</p><p>Regions237</p><p>Array Computation240</p><p>Life,an Example242</p><p>The Problem242</p><p>The Solution242</p><p>How It Works243</p><p>The Philosophv of Life245</p><p>Distinguishing Features of ZPL245</p><p>Regions245</p><p>Statement-Level Indexing245</p><p>Restrictions Imposed by Regions246</p><p>Performance Model246</p><p>Addition by Subtraction247</p><p>Manipulating Arrays of Different Ranks247</p><p>Partial Reduce248</p><p>Flooding249</p><p>The Flooding Principle250</p><p>Data Manipulation,an Example251</p><p>Flood Regions252</p><p>Matrix Multiplication253</p><p>Reordering Data with Remap255</p><p>Index Arrays255</p><p>Remap256</p><p>Ordering Example258</p><p>Parallel Execution of ZPL Programs260</p><p>Role of the Compiler260</p><p>Specifying the Number of Processes261</p><p>Assigning Regions to Processes261</p><p>Array Allocation262</p><p>Scalar Allocation263</p><p>Work Assignment263</p><p>Performance Model264</p><p>Applying the Performance Model：Life265</p><p>Applying the Performance Model：SUMMA266</p><p>Summary of the Performance Model266</p><p>NESL Parallel Language267</p><p>Language Concepts267</p><p>Matrix Product Using Nested Parallelism268</p><p>NESL Complexity Model269</p><p>Chapter Summary269</p><p>Historical Perspective269</p><p>Exercises270</p><p>Chapter 9 Assessing the State of the Art271</p><p>Four Important Properties of Parallel Languages271</p><p>Correctness271</p><p>Performance273</p><p>Scalability274</p><p>Portability274</p><p>Evaluating Existing Approaches275</p><p>POSIX Threads275</p><p>Java Threads276</p><p>OpenMP276</p><p>MPI276</p><p>PGAS Languages277</p><p>ZPL278</p><p>NESL278</p><p>Lessons for the Future279</p><p>Hidden Parallelism279</p><p>Transparent Performance280</p><p>Locality280</p><p>Constrained Parallelism280</p><p>Implicit versus Explicit Parallelism281</p><p>Chapter Summary282</p><p>Historical Perspective282</p><p>Exercises282</p><p>PART 4 Looking Forward283</p><p>Chapter 10 Future Directions in Parallel Programming284</p><p>Attached Processors284</p><p>Graphics Processing Units285</p><p>Cell Processors288</p><p>Attached Processors Summary288</p><p>Grid Computing290</p><p>Transactional Memory291</p><p>Comparison with Locks292</p><p>Implementation Issues293</p><p>Open Research Issues295</p><p>MapReduce296</p><p>Problem Space Promotion298</p><p>Emerging Languages299</p><p>Chapel300</p><p>Fortress300</p><p>X10302</p><p>Chapter Summary304</p><p>Historical Perspective304</p><p>Exercises304</p><p>Chapter 11 Writing Parallel Programs305</p><p>Getting Started305</p><p>Access and Software305</p><p>Hello,World306</p><p>Parallel Programming Recommendations307</p><p>Incremental Development307</p><p>Focus on the Parallel Structure307</p><p>Testing the Parallel Structure308</p><p>Sequential Programming309</p><p>Be Willing to Write Extra Code309</p><p>Controlling Parameters during Testing310</p><p>Functional Debugging310</p><p>Capstone Project Ideas311</p><p>Implementing Existing Parallel Algorithms311</p><p>Competing with Standard Benchmarks312</p><p>Developing New Parallel Computations313</p><p>Performance Measurement314</p><p>Comparing against a Sequential Solution315</p><p>Maintaining a Fair Experimental Setting315</p><p>Understanding Parallel Performance316</p><p>Performance Analysis317</p><p>Experimental Methodology318</p><p>Portability and Tuning319</p><p>Chapter Summary319</p><p>Historical Perspective319</p><p>Exercises320</p><p>Glossary321</p><p>References325</p><p>Index328</p><p></p></div></div><div class="d-rt"><h3>热门推荐</h3><ul><li><a href="/book/1762607.html">1762607.html</a></li><li><a href="/book/1147623.html">1147623.html</a></li><li><a href="/book/2267065.html">2267065.html</a></li><li><a href="/book/2007666.html">2007666.html</a></li><li><a href="/book/493093.html">493093.html</a></li><li><a href="/book/1587417.html">1587417.html</a></li><li><a href="/book/1717808.html">1717808.html</a></li><li><a href="/book/1588139.html">1588139.html</a></li><li><a href="/book/3026246.html">3026246.html</a></li><li><a href="/book/3700390.html">3700390.html</a></li></ul></div></div><div id="footer"><p>Copyright&nbsp;&copy;&nbsp;2025&nbsp;&nbsp;<a href="/list/">最新更新</a></p><p>请使用FDM BitComet qBittorrent uTorrent等BT下载工具，下载本站电子书资源！首推Free Download Manager下载软件。文件页数>标注页数[分册图书除外]</p></div></body></html>